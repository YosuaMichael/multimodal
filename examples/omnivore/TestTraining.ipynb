{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402adda8-64ee-466a-9f25-eca9f627a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_path = \"/datasets01_ontap/imagenet_full_size/061417/\"\n",
    "# kinetics_path = \"/datasets01_ontap/kinetics/070618/\"\n",
    "# sunrgbd_path = \"/data/home/yosuamichael/datasets/SUN_RGBD\"\n",
    "\n",
    "imagenet_path = \"/Users/yosuamichael/Downloads/datasets/mini_omnivore/mini_imagenet\"\n",
    "kinetics_path = \"/Users/yosuamichael/Downloads/datasets/mini_omnivore/mini_kinetics\"\n",
    "sunrgbd_path = \"/Users/yosuamichael/Downloads/datasets/SUN_RGBD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb5888b-5a8e-490e-8c2b-7de3ba9eaf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yosuamichael/.pyenv/versions/3.9.10/envs/explore-3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "import shutil\n",
    "import collections\n",
    "import datetime\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import image_presets\n",
    "import video_presets\n",
    "\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1524abbb-1c9b-4355-93dc-39e22e609b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-15 14:19:27.231036] HELLO\n"
     ]
    }
   ],
   "source": [
    "def lprint(*x):\n",
    "    print(f\"[{datetime.datetime.now()}]\", *x)\n",
    "    \n",
    "lprint(\"HELLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c82b234-3eca-4f23-974c-164d31f28a7f",
   "metadata": {},
   "source": [
    "# Create sunrgbd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c53ea23-8052-45fd-b657-a1fee415be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c57bde-c79d-4e31-8029-cc430b233b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_sunrgbd_image(image_path):\n",
    "#     rgb_dir = os.path.join(image_path, \"image\")\n",
    "#     rgb_path = os.path.join(rgb_dir, os.listdir(rgb_dir)[0])\n",
    "#     img_rgb = PIL.Image.open(rgb_path)\n",
    "#     arr_rgb = np.asarray(img_rgb)\n",
    "    \n",
    "#     # Using depth_bfx, but maybe can also consider just using depth\n",
    "#     depth_dir = os.path.join(image_path, \"depth_bfx\")\n",
    "#     depth_path = os.path.join(depth_dir, os.listdir(depth_dir)[0])\n",
    "#     img_d = PIL.Image.open(depth_path)\n",
    "#     if img_d.mode == \"I\":\n",
    "#         arr_d = (np.asarray(img_d) * 255.99999 / 2**16).astype(np.uint8)\n",
    "    \n",
    "#     arr_rgbd = np.dstack((arr_rgb, arr_d))\n",
    "#     return arr_rgbd\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed621f5-e256-4bdf-87e0-e19c00557e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_rgbd = read_sunrgbd_image(f\"{sunrgbd_path}/kv2/kinect2data/000065_2014-05-16_20-14-38_260595134347_rgbf000121-resize\")\n",
    "# arr_rgbd[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db3ebad8-285d-4a30-b283-be19448a597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sunrgbd_classes_index = {\n",
    "#     \"0\": \"bathroom\", \"1\": \"bedroom\", \"2\": \"classroom\", \"3\": \"computer_room\", \"4\": \"conference_room\", \"5\": \"corridor\",\n",
    "#     \"6\": \"dining_area\", \"7\": \"dining_room\", \"8\": \"discussion_area\", \"9\": \"furniture_store\", \"10\": \"home_office\",\n",
    "#     \"11\": \"kitchen\", \"12\": \"lab\", \"13\": \"lecture_theatre\", \"14\": \"library\", \"15\": \"living_room\", \"16\": \"office\",\n",
    "#     \"17\": \"rest_space\", \"18\": \"study_space\"\n",
    "# }\n",
    "\n",
    "# sunrgbd_classes_set = set(sunrgbd_classes_index.values())\n",
    "# sunrgbd_classes_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5ee9ad-58d8-4b49-982f-27ca313652df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmnivoreSunRgbdDatasets(VisionDataset):\n",
    "    def __init__(self, root, transform = None, target_transform = None, split=\"train\"):\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self._data_dir = Path(self.root) / \"SUNRGBD\"\n",
    "        self._meta_dir = Path(self.root) / \"SUNRGBDtoolbox\"\n",
    "        \n",
    "        if not self._check_exists():\n",
    "            print(f\"data_dir: {self._data_dir}\\nmeta_dir: {self._meta_dir}\")\n",
    "            raise RuntimeError(\"Dataset not found.\")\n",
    "            \n",
    "        self.classes = ['bathroom',\n",
    "             'bedroom',\n",
    "             'classroom',\n",
    "             'computer_room',\n",
    "             'conference_room',\n",
    "             'corridor',\n",
    "             'dining_area',\n",
    "             'dining_room',\n",
    "             'discussion_area',\n",
    "             'furniture_store',\n",
    "             'home_office',\n",
    "             'kitchen',\n",
    "             'lab',\n",
    "             'lecture_theatre',\n",
    "             'library',\n",
    "             'living_room',\n",
    "             'office',\n",
    "             'rest_space',\n",
    "             'study_space'\n",
    "        ]\n",
    "        self.class_to_idx = dict(zip(self.classes, range(len(self.classes))))\n",
    "        \n",
    "        # TODO: Need to change later!\n",
    "        # Currently the file \"sunrgbd_trainval_path.json\" is manually created with a script\n",
    "        # We should create this file from script that is downloaded!\n",
    "        with open(Path(self.root) / \"sunrgbd_trainval_path.json\", \"r\") as fin:\n",
    "            self.trainval_image_dir_map = json.load(fin)\n",
    "            \n",
    "        self.image_dirs = [key for key, value in self.trainval_image_dir_map.items() if value == split]\n",
    "        \n",
    "        \n",
    "    def _check_exists(self):\n",
    "        return self._data_dir.is_dir() and self._meta_dir.is_dir()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_dirs)\n",
    "\n",
    "    def _read_sunrgbd_image(self, image_dir):\n",
    "        rgb_dir = os.path.join(image_dir, \"image\")\n",
    "        rgb_path = os.path.join(rgb_dir, os.listdir(rgb_dir)[0])\n",
    "        img_rgb = PIL.Image.open(rgb_path)\n",
    "        arr_rgb = np.asarray(img_rgb)\n",
    "\n",
    "        # Using depth_bfx, but maybe can also consider just using depth\n",
    "        depth_dir = os.path.join(image_dir, \"depth_bfx\")\n",
    "        depth_path = os.path.join(depth_dir, os.listdir(depth_dir)[0])\n",
    "        img_d = PIL.Image.open(depth_path)\n",
    "        if img_d.mode == \"I\":\n",
    "            arr_d = (np.asarray(img_d) * 255.99999 / 2**16).astype(np.uint8)\n",
    "\n",
    "        arr_rgbd = np.dstack((arr_rgb, arr_d))\n",
    "        return arr_rgbd\n",
    "    \n",
    "    def _get_sunrgbd_scene_class(self, image_dir):\n",
    "        with open(os.path.join(image_dir, \"scene.txt\"), \"r\") as fin:\n",
    "            scene_class = fin.read().strip()\n",
    "        return scene_class\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return tuple of image (H W C==4) and scene class index\n",
    "        image_dir = self.image_dirs[idx]\n",
    "        x_rgbd = torch.tensor(self._read_sunrgbd_image(image_dir), dtype=torch.uint8)\n",
    "        x_rgbd = x_rgbd.permute(2, 0, 1) # H W C -> C H W\n",
    "        scene_class = self._get_sunrgbd_scene_class(image_dir)\n",
    "        scene_idx = self.class_to_idx[scene_class]\n",
    "        \n",
    "        if self.transform:\n",
    "            x_rgbd = self.transform(x_rgbd)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            scene_idx = self.target_transform(scene_idx)\n",
    "            \n",
    "        return x_rgbd, scene_idx\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8137a21-6b40-49e2-ad16-f11c8e14426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunrgbd_train = OmnivoreSunRgbdDatasets(root=sunrgbd_path, split=\"train\")\n",
    "sunrgbd_val = OmnivoreSunRgbdDatasets(root=sunrgbd_path, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "915ca342-03ad-48b9-8e07-34d82e88c321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4659"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sunrgbd_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576fa3d2-5842-4294-b534-0a1494936eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 530, 730]), 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sunrgbd_train[0][0].shape, sunrgbd_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab901aa-d166-44dd-ac8f-6d5c90824b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4845, 4845)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(sunrgbd_train)\n",
    "\n",
    "len(iter(dl)), len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b2566-d0e2-4186-92ae-55e3e51ff029",
   "metadata": {},
   "source": [
    "# Create mini imagenet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b0ba8a-6971-44dd-9d64-0b2386cb2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmnivoreImageFolder(torchvision.datasets.folder.ImageFolder):\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = super().__getitem__(idx)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66db001d-0a56-4145-b5ab-377fc0ac6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet = OmnivoreImageFolder(f\"{imagenet_path}/train\", T.PILToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8426710-95cd-4e13-b0b4-a97f45b0f110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 250, 250])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eebc64a0-d716-48a9-8ebb-713ca864f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagenet.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28f4d050-43c3-4a82-b39c-20f408ea2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2path = {}\n",
    "for sample in imagenet.samples:\n",
    "    path, label = sample\n",
    "    label2path[label] = label2path.get(label, []) + [path]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2403a350-5719-4430-bd19-0c2ecab0ea75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "915a430f-5a7f-4d65-a078-f044737eba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_imagenet_root = \"/data/home/yosuamichael/datasets/mini_imagenet/\"\n",
    "# ori_imagenet_prefix = \"/datasets01_ontap/imagenet_full_size/061417/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c10f22-0f8a-4b35-af9f-3649e5f4a78d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Take 5 images for every label\n",
    "# num_per_label = 5\n",
    "# for label in label2path:\n",
    "#     for i in range(num_per_label):\n",
    "#         path = label2path[label][i]\n",
    "#         new_path = os.path.join(new_imagenet_root, path[len(ori_imagenet_prefix):])\n",
    "#         new_dir = os.path.dirname(new_path)\n",
    "#         os.makedirs(new_dir, exist_ok=True)\n",
    "#         shutil.copy(path, new_path)\n",
    "#     print(f\"Finish label: {label}\")\n",
    "    \n",
    "# print(\"FINISH ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6fc7d85-2895-4a3f-8ff2-27d15f4b4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.dirname(\"/data/home/yosuamichael/datasets/mini_imagenet/train/n01440764/n01440764_10026.JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49dded5b-4b0d-4189-9e1e-a96564114b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_imagenet_train = OmnivoreImageFolder(f\"{imagenet_path}/train\", T.PILToTensor())\n",
    "mini_imagenet_val = OmnivoreImageFolder(f\"{imagenet_path}/val\", T.PILToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4264a8dd-9931-4637-a5ea-6c15aac8bf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_imagenet_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f5c04-2755-4e54-bea5-9eb3d807cef8",
   "metadata": {},
   "source": [
    "# Create mini kinetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98003e30-d0d1-431a-aa97-463b4af8b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent to do this because it is very slow!\n",
    "# kinetics = torchvision.datasets.kinetics.Kinetics(\"/datasets01_ontap/kinetics/070618/train_avi-480p\", frames_per_clip=32, frame_rate=16, _legacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c95cf04-9588-4bf9-9bff-932436a8789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_counter = 0\n",
    "\n",
    "# mini_kinetics_folder = \"/data/home/yosuamichael/datasets/mini_kinetics/\"\n",
    "# ori_kinetics_prefix = \"/datasets01_ontap/kinetics/070618/\"\n",
    "\n",
    "# folder_counter = collections.Counter()\n",
    "# num_file_per_folder = 3\n",
    "\n",
    "# for root, folders, filenames in os.walk(\"/datasets01_ontap/kinetics/070618/val_avi-480p/\"):\n",
    "#     for filename in filenames:\n",
    "#         ori_filepath = os.path.join(root, filename)\n",
    "#         new_filepath = os.path.join(mini_kinetics_folder, ori_filepath[len(ori_kinetics_prefix):])\n",
    "#         new_folder = os.path.dirname(new_filepath)\n",
    "#         if folder_counter[new_folder] >= num_file_per_folder:\n",
    "#             continue\n",
    "#         os.makedirs(new_folder, exist_ok=True)\n",
    "#         shutil.copy(ori_filepath, new_filepath)\n",
    "#         folder_counter[new_folder] += 1\n",
    "        \n",
    "#         all_counter += 1\n",
    "#         if all_counter % 100 == 0:\n",
    "#             lprint(f\"all_counter: {all_counter}\")\n",
    "\n",
    "# print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca0b87c2-58e0-46ca-be74-1f5875273424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmnivoreKinetics(torchvision.datasets.kinetics.Kinetics):\n",
    "    def __getitem__(self, idx):\n",
    "        video, audio, label = super().__getitem__(idx)\n",
    "        return video, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c355236-d642-43a7-8ea0-9c29ac51bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_kinetics_train = OmnivoreKinetics(\n",
    "#     f\"{kinetics_path}/train\", \n",
    "#     frames_per_clip=32, frame_rate=32, step_between_clips=32, \n",
    "#     _legacy=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae072534-99ab-4649-9cd6-9691e105c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lprint(\"Start\")\n",
    "# mini_kinetics_val = OmnivoreKinetics(\n",
    "#     f\"{kinetics_path}/val\", \n",
    "#     frames_per_clip=32, frame_rate=32, step_between_clips=32, \n",
    "#     _legacy=True\n",
    "# )\n",
    "\n",
    "# lprint(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60d9d042-57a8-4b3f-b54b-41dfa28b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(mini_kinetics_train[5]), mini_kinetics_train[5][0].shape, mini_kinetics_train[5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdc2aef2-b39c-497b-b535-657f14b0eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_kinetics_train[0][0].shape, mini_kinetics_train[0][1], mini_kinetics_train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517dcc90-16b0-4003-9dad-1ce5a603f23b",
   "metadata": {},
   "source": [
    "# Create class to concat data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1b7f003-99e1-4a75-b272-11f77cdb610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatIterable:\n",
    "    def __init__(self, iterables, output_keys, repeat_factors, seed=42): \n",
    "        self.iterables = iterables\n",
    "        self.output_keys = output_keys\n",
    "        self.repeat_factors = repeat_factors\n",
    "        self.seed = seed\n",
    "        self.num_iterables = len(self.iterables)\n",
    "        assert self.num_iterables == len(output_keys)\n",
    "        assert self.num_iterables == len(repeat_factors)\n",
    "        \n",
    "        \n",
    "        # The iterator len is adjusted with repeat_factors\n",
    "        self.iterator_lens = [int(repeat_factors[i] * len(itb)) for i, itb in enumerate(self.iterables)]\n",
    "        self.max_total_steps = sum(self.iterator_lens)\n",
    "        self.indices = None\n",
    "        self.iterators = None\n",
    "        \n",
    "        # self.step_counter == None indicate that self.indices are not yet initialized\n",
    "        self.step_counter = None\n",
    "        \n",
    "    def init_indices(self, epoch=0, shuffle=False):\n",
    "        # We should initiate indices for each epoch, especially if we want to shuffle\n",
    "        self.step_counter = 0\n",
    "    \n",
    "        self.iterators = [iter(dl) for dl in self.iterables]\n",
    "        self.indices = torch.cat([torch.ones(self.iterator_lens[i], dtype=torch.int32) * i for i in range(self.num_iterables)])\n",
    "        assert self.max_total_steps == len(self.indices)\n",
    "        \n",
    "        if shuffle:\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(self.seed + epoch)\n",
    "            shuffle_indices = torch.randperm(len(self.indices), generator=g)\n",
    "            self.indices = self.indices[shuffle_indices]\n",
    "            \n",
    "    def __next__(self):\n",
    "        if self.step_counter == None:\n",
    "            # Initiate the indices without shuffle as default!\n",
    "            self.init_indices()\n",
    "        if self.step_counter >= self.max_total_steps:\n",
    "            raise StopIteration\n",
    "        \n",
    "        idx = self.indices[self.step_counter]\n",
    "        output_key = self.output_keys[idx]\n",
    "        print(idx)\n",
    "        try:\n",
    "            batch = next(self.iterators[idx])\n",
    "        except StopIteration:\n",
    "            # We cycle over the data_loader to the beginning. This can happen when repeat_factor > 1\n",
    "            # Take note that in this case we always use same shuffling from same data_loader in an epoch\n",
    "            self.iterators[idx] = iter(self.iterables[idx])\n",
    "            batch = next(self.iterators[idx])\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        # Return batch and output_key\n",
    "        return batch, output_key\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.max_total_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a465bd3-2d76-4559-97a2-29ef094e1141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, dtype=torch.int32)\n",
      "(1, 'a')\n"
     ]
    }
   ],
   "source": [
    "a = ConcatIterable( [[1], [1,2], [1,2,3]], ['a', 'b', 'c'], [2,1,1] )\n",
    "a.indices\n",
    "\n",
    "# a.init_indices(1, True)\n",
    "# a.indices\n",
    "\n",
    "for x in a:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "514c49e8-8d32-46be-b088-3fce7fc666e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = ConcatIterable([mini_imagenet_train, mini_kinetics_train], ['image', 'video'], [1,1])\n",
    "\n",
    "# a.init_indices(epoch=0, shuffle=True)\n",
    "# for batch, key in a:\n",
    "#     print(key)\n",
    "#     break\n",
    "    \n",
    "# print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1e908d4-908e-4451-a391-a104979f8ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mini_kinetics_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(mini_imagenet_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmini_kinetics_train\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mini_kinetics_train' is not defined"
     ]
    }
   ],
   "source": [
    "len(mini_imagenet_train) + len(mini_kinetics_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b3239-1749-4db3-b71a-7f0b4bb87b18",
   "metadata": {},
   "source": [
    "# Create dataset with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4afe7614-a0f7-4c04-b868-43d042add3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO tensor([0])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(2, (1,))\n",
    "if x:\n",
    "    print(\"YES\", x)\n",
    "else:\n",
    "    print(\"NO\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e554c0ea-93bb-4b7d-a9ae-b1fca26f01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_crop_size = 224\n",
    "\n",
    "imagenet_train_preset = image_presets.ImageNetClassificationPresetTrain(crop_size=train_crop_size, interpolation=InterpolationMode.BICUBIC,\n",
    "                            auto_augment_policy=\"ra\", random_erase_prob=0.25, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c2df21d-1e49-4dba-946c-96e5c3e66809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_imagenet_train = OmnivoreImageFolder(f\"{imagenet_path}/train\", imagenet_train_preset)\n",
    "mini_imagenet_train[0][0].shape  # C, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7babfea3-3446-494f-8ab8-88366cd89f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:06<00:00, 10.87it/s]\n",
      "/Users/yosuamichael/manual_build_lib/pytorch/vision/torchvision/datasets/video_utils.py:223: UserWarning: There aren't enough frames in the current video to get a clip for the given clip length and frames between clips. The video (and potentially others) will be skipped.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_resize_size = 256\n",
    "train_crop_size = 224\n",
    "video_train_preset = video_presets.VideoClassificationPresetTrain(crop_size=train_crop_size, resize_size=train_resize_size, )\n",
    "\n",
    "mini_kinetics_train = OmnivoreKinetics(\n",
    "    f\"{kinetics_path}\", \n",
    "    frames_per_clip=32, frame_rate=16, step_between_clips=16, \n",
    "    split=\"train\", transform= video_train_preset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cf34f3b-0072-42f5-9089-68706dfd896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yosuamichael/manual_build_lib/pytorch/vision/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 224, 224])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_kinetics_train[1][0].shape  # C, D, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bb756a5-a504-4d68-94d7-4f0b04bb3108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create RandAugment3d, see: \n",
    "# - https://www.internalfb.com/code/fbsource/[f1a98f41bcce]/fbcode/deeplearning/projects/omnivore/vissl/data/ssl_transforms/rand_aug_3d.py\n",
    "# - https://github.com/pytorch/vision/blob/main/torchvision/transforms/autoaugment.py#L287\n",
    "# Basically do normal augmentation for those operation that involve geometry (shear, translate, etc)\n",
    "# and only apply color operation to rgb without changing the depth\n",
    "# __DONE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83d478aa-7b16-4318-a213-0b93cb491523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import depth_presets\n",
    "\n",
    "train_crop_size = 224\n",
    "\n",
    "depth_train_preset = depth_presets.DepthClassificationPresetTrain(crop_size=train_crop_size, interpolation=InterpolationMode.NEAREST,\n",
    "                            random_erase_prob=0.25, )\n",
    "\n",
    "mini_sunrgbd_train = OmnivoreSunRgbdDatasets(root=sunrgbd_path, split=\"train\", transform=depth_train_preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecd628fb-ef5d-4d77-8c1e-d9eb54a1bdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 224, 224])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = mini_sunrgbd_train[0][0]\n",
    "a.shape # C, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6028fb3-c5fb-41d7-89e3-2bf875b33bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try concat data_loader\n",
    "\n",
    "def get_single_data_loader_from_dataset(train_dataset): \n",
    "    batch_size = 8\n",
    "    \n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=True,\n",
    "        collate_fn=torch.utils.data.dataloader.default_collate,\n",
    "    )\n",
    "\n",
    "    return train_data_loader\n",
    "\n",
    "image_data_loader = get_single_data_loader_from_dataset(mini_imagenet_train)\n",
    "video_data_loader = get_single_data_loader_from_dataset(mini_kinetics_train)\n",
    "depth_data_loader = get_single_data_loader_from_dataset(mini_sunrgbd_train)\n",
    "\n",
    "\n",
    "data_loader = ConcatIterable([image_data_loader, video_data_loader, depth_data_loader], ['image', 'video', 'depth'], [1,0,0])\n",
    "data_loader.init_indices(epoch=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd646b1f-d5b1-4acc-a5c5-3b98da5e20a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 3, 224, 224]), 'image')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(data_loader)\n",
    "x[0][0].shape, x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abf42657-4bd2-4b80-afc2-7d03353b6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video: B, C, D, H, W\n",
    "# Depth: B, C, H, W\n",
    "# Image: B, C, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fadeb8a-9943-4037-b620-c7dbb3927250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_func = depth_presets.RGBToFloatAndDepthNorm(max_depth=75, clamp_max_before_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8270d0d6-1578-4fee-b411-50978267d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sums = []\n",
    "# sumsqs = []\n",
    "# pixels = []\n",
    "# for_stds = []\n",
    "\n",
    "# for i in range(len(sunrgbd_train)):\n",
    "#     img = norm_func(sunrgbd_train[i][0])\n",
    "#     sums.append(torch.sum(img.float(), dim=[1,2]))\n",
    "#     sumsqs.append(torch.sum(img.float()**2, dim=[1,2]))\n",
    "#     pixels.append(img.shape[1] * img.shape[2])\n",
    "#     if i % 1000 == 0:\n",
    "#         lprint(i)\n",
    "   \n",
    "# lprint(\"FINISHED\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16beb892-3121-46a2-892e-2d2ad973d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce the sums and sumsqs and pixels\n",
    "# total_sums = torch.tensor([0., 0., 0., 0.])\n",
    "# total_sumsqs = torch.tensor([0., 0., 0., 0.])\n",
    "# total_pixels = 0\n",
    "\n",
    "# for i in range(len(means)):\n",
    "#     total_sums += sums[i]\n",
    "#     total_sumsqs += sumsqs[i]\n",
    "#     total_pixels += pixels[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7ab8eb9-32b0-4496-8f34-a433f12828da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = total_sums / total_pixels\n",
    "\n",
    "# variance = total_sumsqs / total_pixels - means ** 2\n",
    "\n",
    "# stdev = variance ** 0.5\n",
    "\n",
    "# print(f\"mean: {mean}, stdev: {stdev}\")\n",
    "\n",
    "# mean: tensor([0.4975, 0.4648, 0.4412, 0.8049]), stdev: tensor([0.2781, 0.2873, 0.2910, 0.2116])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55b5f8-0c53-433a-b129-a654649aec16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e4b14f9-5ff8-43d1-bf9a-7f8a13e7966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmultimodal.models.omnivore as omnivore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be26865b-54ec-4d6b-8c5f-f8f104d48064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc394620-3dcf-4966-8991-54de2b2bbe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28945041\n"
     ]
    }
   ],
   "source": [
    "m = omnivore.omnivore_swin_t()\n",
    "\n",
    "print(count_parameters(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3cf27-51cd-4ca7-9c1d-43389ab83cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
